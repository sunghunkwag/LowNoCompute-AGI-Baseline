# LowNoCompute-AI-Baseline Configuration
# Configuration for experience-based reasoning with SSM + MAML

# LightweightSSM Configuration
ssm:
  input_dim: 1
  hidden_dim: 8
  output_dim: 1
  
# MinimalMAML Configuration  
maml:
  inner_lr: 0.01        # Learning rate for inner loop (fast adaptation)
  outer_lr: 0.001       # Learning rate for outer loop (meta-learning)
  inner_steps: 3        # Number of gradient steps in inner loop
  finite_diff_eps: 1e-5 # Epsilon for finite difference gradients

# ExperienceBuffer Configuration
experience_buffer:
  max_size: 200                    # Maximum number of experiences to store
  experience_batch_size: 10        # Number of experiences to sample during adaptation
  
# Meta-Training Configuration
meta_training:
  num_episodes: 10      # Number of meta-training episodes
  tasks_per_batch: 4    # Number of tasks per meta-update batch
  task_types:           # Types of synthetic tasks to generate
    - "sine"
    - "linear"
  
# Task Generation Configuration
task_generation:
  support_samples: 5    # Number of support samples per task
  query_samples: 10     # Number of query samples per task
  input_range: [-2, 2]  # Range for input sampling
  noise_std: 0.0       # Standard deviation of noise to add
  
# Test-time Adaptation Configuration
test_adaptation:
  adaptation_steps: 3           # Steps for test-time adaptation
  compare_with_without: true    # Compare adaptation with/without experience buffer
  
# Logging Configuration
logging:
  verbose: true
  print_interval: 5     # Print progress every N episodes
  show_adaptation_demo: true
  
# Random Seed Configuration
seed:
  numpy_seed: 42
  
# Performance Configuration
performance:
  use_float64: true     # Use float64 for numerical stability (recommended)
  batch_finite_diff: false  # Whether to batch finite difference computations